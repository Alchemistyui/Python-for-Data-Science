{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning is ...\n",
    "\n",
    "- focus on computer systems that can learn from data; \n",
    "- can learn specific task on its own(important feature); \n",
    "- discover hidden patterns and trends\n",
    "- lead to data driven funcs/decisions for the task\n",
    "\n",
    "# ML Applications\n",
    "\n",
    "- credit card fraud detection\n",
    "- hand-written digit recognition\n",
    "- recommendation on webs\n",
    "\n",
    "# ML and Data Science\n",
    "\n",
    "- Data Mining: find patterns in database, etc.\n",
    "- Predictive analytics: analyze data in order to predict feature outcomes(eg: predict customer purchace)\n",
    "- Big data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories of ML\n",
    "\n",
    "- Supervised: target is provided/labeled data\n",
    "    - Classification: predict category of input\n",
    "    - Regression: predict numeric value\n",
    "- Unsupervised: target is unknown/unlabeled data\n",
    "    - Cluster Analysis: organize similar items into groups\n",
    "    - Association Analysis: come up with a set of rules to capture associations btw items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology related to ML\n",
    "\n",
    "\n",
    "## Terms to describe data\n",
    "\n",
    "Sample/record/example/row/instance/observation: instance or example of an entity\n",
    "\n",
    "Variable/feature/column/dimension/attribute/field: diff info pieces of the sample\n",
    "\n",
    "**data types of variable are usually numeric and categorical(others can be string or date)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "\n",
    "- open source lib for end-to-end ML\n",
    "- build on Numpy, Scipy & matplotlib\n",
    "- active community for develop\n",
    "- support the whole process of machine learning, data cleaning & data transformation(utility funcs and API)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Classification Algorithms\n",
    "\n",
    "- kNN: k nearest neighbours - input data has similar features likely belong to the same class\n",
    "- Decision Tree: use tree structure to capture multiple decision paths\n",
    "- Naive Bayes: probablistic approach to classification(Bayes Theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree for classification\n",
    "\n",
    "idea: split data into subset, that each subset belongs to only one class - divide the input space to the pure region\n",
    "\n",
    "*completely pure data in subset is impossible, so the goal is to make subset as pure as possible*\n",
    "\n",
    "each subset contains as many sample as possible of a single class - decision boundry\n",
    "\n",
    "## Decision Tree structure\n",
    "\n",
    "a hierarchical structure with directional edges and nodes\n",
    "\n",
    "top node - root node\n",
    "\n",
    "bottom mode - leaf node(associated with a class label)\n",
    "\n",
    "other node - internal node\n",
    "\n",
    "\n",
    "how to make decision: traversing the dicision tree through the root node, each internal node's answer determines which branch to traverse to, until reach the leaf node\n",
    "\n",
    "depth: # of edges from root node to that node\n",
    "size: # of nodes in the tree\n",
    "\n",
    "## Desicion Tree construction / Tree induction\n",
    "\n",
    "- start all samples at the root node\n",
    "- partition samples based on input to create purest subsets(each subset contains as many samples as possible belonging to just one class)\n",
    "- repeat to partition data into successively purer subsets\n",
    "\n",
    "**Greedy approach - the best way to split particular portion of the data**\n",
    "\n",
    "### impurity measure \n",
    "\n",
    "- Gini index: lower = higher pure, higher = less pure, so the dicision tree will select the methods that minimize the gini index\n",
    "- which variable to split on?: test all variables to determine the best way to split the nodes\n",
    "\n",
    "## When to stop splitting a node?\n",
    "\n",
    "- all(or x% of) samples in the node have same class label\n",
    "- number of samples in node reaches minimum\n",
    "- improvement in impurity measure is smaller than threshold(too small to make much of the difference in clsfication results)\n",
    "- max tree depth is reached(control the complexity of the tree)\n",
    "\n",
    "\n",
    "## Decision Tree for Classification\n",
    "\n",
    "- resulting tree is simple and easy to interpret(to see what variable is important)\n",
    "- induction is computationally inexpensive\n",
    "- greedy approach doesn't guarantee best solution\n",
    "- rectilinear decision boundaries(can't solve more complex decision boudaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis\n",
    "\n",
    "\n",
    "### examples:\n",
    "- segment customer base into group\n",
    "- characterize diff weather patterns for a region\n",
    "- group new articles into topics to identify the trendy topic in the day\n",
    "- discover crime hot spots\n",
    "\n",
    "\n",
    "### features:\n",
    "- divide data into clusters\n",
    "- similar items are placed in same cluster\n",
    "- intra-cluster differences are minimized\n",
    "- inter_cluster differences are maximized\n",
    "\n",
    "\n",
    "### metrics to measure similarity:\n",
    "- Euclidean Distance: distance along a straight line btw the two points\n",
    "- Manhattan Distance: straightly vertical and horizontal path(along the axis)\n",
    "- Cosine similarity: cosine of the angle btw 2 points\n",
    "\n",
    "**normalizing input values!! is very important so that no one values dorminate the similarity calculation**\n",
    "\n",
    "### normalize input variables\n",
    "\n",
    "scaling is very important when the variables has very diff scales(eg: weight & height) -> scale all variables to a common value range\n",
    "\n",
    "### cluster analysis notes\n",
    "\n",
    "- unsupervised task(no labeled data)\n",
    "- there is no correct clustering results(depend on the applications)\n",
    "- clusters don't come with labels: you don't know what each cluster means\n",
    "\n",
    "*interpretation and analysis are required to make sense of clustering result!*\n",
    "\n",
    "### uses of cluster result\n",
    "\n",
    "- data segmentation: analysis of each segment can provide insight\n",
    "- categories for classifying new data: new sample assigned to closest cluster\n",
    "- labeled data for classification: can provide more (unlabel) data for classification, since once a sample's label is determined, the cluster's label are determined\n",
    "- basis for anomaly detection: cluster outliers are anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm\n",
    "\n",
    "### step:\n",
    "\n",
    "- select k initial centroids(cluster centers)(randomly selected)\n",
    "- Repeat(until stopping criterion is reached)\n",
    "    - assign each sample to the closest centroid\n",
    "    - calculate mean of cluster to determin new centroid\n",
    "    \n",
    "### Choosing initial centroids\n",
    "\n",
    "Issue: final cluster are sensitive to initial centroids(diff initial centroids can cause diff cluster lastly)\n",
    "\n",
    "Solution: run k-means multiple times with diff random initial centroids, and choose the best result\n",
    "\n",
    "\n",
    "### Evaluate result\n",
    "\n",
    "error = distance btw sample and centroid\n",
    "\n",
    "squared error = error ** 2\n",
    "\n",
    "sum of squared error btw all sample & centroid pair\n",
    "\n",
    "sum over all clusters\n",
    "\n",
    "-> WSSE: Within-Cluster Sum of Squared Error\n",
    "\n",
    "#### WSSE the smaller the better?  ->   No!\n",
    "\n",
    "WSSE is better numerically doesn't means more 'correct' for clustering, since larger k always leads to smaller WSSE\n",
    "\n",
    "*only use WSSE for the same value for k and generated from the same dataset*\n",
    "\n",
    "*small WSSE may not be the best solution -> interpretation and domain knowledge is crucial*\n",
    "\n",
    "\n",
    "### Choose the value for k\n",
    "\n",
    "Approaches:\n",
    "- Visualization: find the natural grouping of the sample(scatter plot, dimension reduction)\n",
    "- Application-Dependent: domain knowledge\n",
    "- Data-Driven: calculate some metrics for different value of k to determine the best selection(eg: Elbow Methods)\n",
    "\n",
    "\n",
    "### Elbow Method\n",
    "\n",
    "\n",
    "increase k and see how the WSSE changed, the best k is where the slope become slow\n",
    "\n",
    "*this method is not always effective especially for complex data, and it may suggest multiple values for k, so this can be a guideline for the range of values to try for k*\n",
    "\n",
    "\n",
    "### Stopping criteria\n",
    "\n",
    "- no change to centroid\n",
    "- numbers of samples changing cluster is below threshold\n",
    "\n",
    "\n",
    "### Interpreting results\n",
    "\n",
    "compare centroids for each cluster to see how cluster different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "\n",
    "give input variables, predict numeric values\n",
    "\n",
    "- Training data: Adjust model parameters\n",
    "- Validation data: Determine when to stop training(avoid overfitting), estimate generalization performance\n",
    "- Test data: Evaluate performance on new data\n",
    "\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "training: adjust regression params(eg: slope & y-intercept)\n",
    "\n",
    "-> using Least Squares method: goal is find the regression line that makes sum of residuals as small as possible\n",
    "\n",
    "- simple linear regression: just one varaibles\n",
    "- multiple linear regression: more than one variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
